{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "from math import *\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 32: Likelihood Ratio Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we introduced Likelihood Ratio tests. Recall that the point of a likelihood ratio test is to compare the likelihood function under a hypothesized value of the parameter with the liklihood function at its maximum. Instead of looking at the ratio $\\Lambda$ itself, we often consider $-2\\log \\Lambda$ instead, since it has a handy distribution. \n",
    "\n",
    "### Example 1: Exponential Distribution\n",
    "\n",
    "Suppose $X_1,X_2,...,X_n$ is an iid sequence of random variables from the exponential distribution with unknown parameter $\\lambda$. Recall that the maximum likelihood estimate of $\\lambda$ is $1\\over\\bar{X}$. We collect a random sample of size 20 and want to test the hypothesis $H_0: \\lambda = 3$ vs $H_1: \\lambda \\neq 3$. Using the data in the python box below, conduct a likelihood ratio test on this hypothesis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data=np.array([0.18,0.277,0.105,0.126,0.225,0.026,0.123,0.423,0.006,0.281,0.050,0.692,0.105,0.275,0.346,0.079,0.045,0.222,0.063,0.281])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First find the expression for the likelihood ratio:*\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\Lambda&=\\frac{L(\\lambda_0\\mid\\textbf{x})}{L(\\hat{\\lambda}_{ML}\\mid\\textbf{x})}\\\\\n",
    "&=\\frac{\\prod_{i=1}^n f(x_i;\\lambda=3)}{\\prod_{i=1}^n f(x_i;\\lambda=\\frac{1}{\\bar{X}})}\\\\\n",
    "&=\\frac{\\prod_{i=1}^n\\left(\\lambda e^{-\\lambda x_i}\\right)_{\\lambda=3}}{\\prod_{i=1}^n\\left(\\lambda e^{-\\lambda x_i}\\right)_{\\lambda=\\frac{1}{\\bar{X}}}}\\\\\n",
    "&=\\frac{\\left(\\lambda^n e^{\\left(-\\lambda\\sum_{i=1}^n x_i\\right)}\\right)_{\\lambda=3}}{\\left(\\lambda^n e^{\\left(-\\lambda\\sum_{i=1}^n x_i\\right)}\\right)_{\\lambda=\\frac{1}{\\bar{X}}}}\\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Likelihood Ratio of null hypothesis is 0.0944569427967814\n",
      "The p-value for the null hypothesis is 0.029827229194775096\n"
     ]
    }
   ],
   "source": [
    "# Get values to plug in\n",
    "xmean = np.mean(my_data)\n",
    "xsum = np.sum(my_data)\n",
    "ltest = 3\n",
    "lmax = 1/xmean\n",
    "n=20\n",
    "\n",
    "# Calculate the likelihood ratio\n",
    "L=(ltest**n * e**(-ltest*xsum) ) / (lmax**n * e**(-lmax*xsum) )\n",
    "print(\"The Likelihood Ratio of null hypothesis is \" + str(L))\n",
    "\n",
    "# Calculate the p-value of -2log(L) under the chi-squared distribution\n",
    "p = 1-stats.chi2.cdf(x=-2*log(L), df=1)\n",
    "print(\"The p-value for the null hypothesis is \" + str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since the likelihood ratio of the null hypothesis is very small, with a p-value less than 0.05, we have evidence to reject the null hypothesis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power\n",
    "\n",
    "Suppose that the true value of $\\lambda$ is 5. Let's determine the power of this test. Let $n=20$. Then determine the power if $n=50$. Remember, power is the probability of correctly rejecting the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find what value of $-2 \\log \\Lambda$ would lead you to reject $H_0$. This is sometimes called the critical value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.841458820694124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcrit = 0.05 # this is the p-value below which we would reject H0\n",
    "critValue = stats.chi2.ppf(q=1-pcrit,df=1)\n",
    "critValue # any test statistic ABOVE this value will indicate a rejected null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, obtain the power. Obtain a sample of size 20 from the true population and obtain the value of $-2\\log \\Lambda$ for this sample. Repeat many times and determine how often you reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6054"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltrue=5\n",
    "ltest=3\n",
    "rejections = make_array()\n",
    "\n",
    "for _ in np.arange(10000):\n",
    "    sample=stats.expon.rvs(scale=1/ltrue, size=n) # get sample\n",
    "\n",
    "    xmean = np.mean(sample) # get values to plug into L\n",
    "    xsum = np.sum(sample)\n",
    "    lmax = 1/xmean\n",
    "\n",
    "    L=(ltest**n * e**(-ltest*xsum) ) / (lmax**n * e**(-lmax*xsum) )\n",
    "    testStat = -2*log(L) # Get test statitic\n",
    "    rejections = np.append(rejections, testStat>critValue) # record whether hypothesis gets rejected\n",
    "    \n",
    "np.mean(rejections) # fraction of tests that correctly reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for a sample size of 50. What do you expect to happen to power? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9954"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejections = make_array()\n",
    "n=50\n",
    "\n",
    "for _ in np.arange(10000):\n",
    "    sample=stats.expon.rvs(scale=1/ltrue, size=n) # get sample\n",
    "\n",
    "    xmean = np.mean(sample) # get values to plug into L\n",
    "    xsum = np.sum(sample)\n",
    "    lmax = 1/xmean\n",
    "\n",
    "    L=(ltest**n * e**(-ltest*xsum) ) / (lmax**n * e**(-lmax*xsum) )\n",
    "    testStat = -2*log(L) # Get test statitic\n",
    "    rejections = np.append(rejections, testStat>critValue) # record whether hypothesis gets rejected\n",
    "    \n",
    "np.mean(rejections) # fraction of tests that correctly reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The power is much greater, indicating that an incorrect rejection of the null hypothesis is now very unlikely.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Different Test\n",
    "\n",
    "We've explored hypothesis tests in this class before. Taking advantage of our computing power, we don't have to rely on test statistics with asymptotic distributions. Let's conduct a more direct hypothesis test using simulation. Recall:\n",
    "\n",
    "$$\n",
    "H_0: \\lambda = 3\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_1: \\lambda \\neq 3\n",
    "$$\n",
    "\n",
    "Pick a different test statistic. Obtain an empirical distribution of that test statistic under $H_0$. Next, find the $p$-value by determining how often this test statistic is at or further away from the test statistic derived from the sample. Remember that this is a two-sided test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value of the observed data is 0.0156\n"
     ]
    }
   ],
   "source": [
    "# New Test Statistic: abs(3 - 1/mean of sample)\n",
    "    # small values indicate sample is in accordance with H0\n",
    "    # large values indicate sample is not in accordance with H0\n",
    "\n",
    "# PART 1: empirical distribution of test stat under H0\n",
    "\n",
    "testStats = make_array()\n",
    "n=20\n",
    "for _ in np.arange(10000):\n",
    "    sample=stats.expon.rvs(scale=1/ltest, size=n) # get sample using H0 lambda\n",
    "    testStats = np.append(testStats, np.abs(ltest - 1/np.mean(sample))) # record test statistic\n",
    "    \n",
    "# PART 2: get p-value of observed test statistic\n",
    "obsTest = np.abs(ltest - 1/np.mean(my_data))\n",
    "p = np.sum(testStats>obsTest) / len(testStats)\n",
    "print('The p-value of the observed data is ' + str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the $p$-value compare to the LRT $p$-value? I wonder how the power of this test compares to our LRT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The $p$-value of this test was somewhat less than the LRT $p$-value, leading to the same conclusion of rejecting the null hypothesis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power\n",
    "\n",
    "Let's figure out the power of this test. First, determine for what values of the test statistic would lead us to reject $H_0$. These values can be referred to as your rejection region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4730755714624273"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0499"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the value in testStats corresponding to the p-th percentile,\n",
    "    # order testStats, and find the value in the p*len(testStats) position\n",
    "critValue = np.sort(testStats)[int((1-pcrit)*len(testStats))]\n",
    "critValue\n",
    "np.sum(testStats>critValue) / len(testStats) # check by calculating p-value of critical value\n",
    "# Our rejection region will be any test statistic GREATER then the critical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, determine the power of this test. Like in the LRT case, obtain a sample of size 20 and obtain the test statistic. Repeat many times and see how often your test statistic is in your rejection region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7165"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejections = make_array()\n",
    "\n",
    "for _ in np.arange(10000):\n",
    "    sample=stats.expon.rvs(scale=1/ltrue, size=n) # get sample using TRUE lambda\n",
    "    testStat = np.abs(ltest - 1/np.mean(sample))\n",
    "    rejections = np.append(rejections, testStat>critValue) # record whether hypothesis gets rejected\n",
    "\n",
    "np.mean(rejections) # fraction of tests that correctly reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for a sample size of 50. Note that you will have to obtain new critical values in order to do this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8640884631745305"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9737"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=50\n",
    "# Get new critical value\n",
    "testStats = make_array()\n",
    "for _ in np.arange(10000):\n",
    "    sample=stats.expon.rvs(scale=1/ltest, size=n) # get sample using H0 lambda\n",
    "    testStats = np.append(testStats, np.abs(ltest - 1/np.mean(sample))) # record test statistic\n",
    "critValue = np.sort(testStats)[int((1-pcrit)*len(testStats))]\n",
    "critValue\n",
    "\n",
    "# Retest power\n",
    "rejections = make_array()\n",
    "\n",
    "for _ in np.arange(10000):\n",
    "    sample=stats.expon.rvs(scale=1/ltrue, size=n) # get sample using TRUE lambda\n",
    "    testStat = np.abs(ltest - 1/np.mean(sample))\n",
    "    rejections = np.append(rejections, testStat>critValue) # record whether hypothesis gets rejected\n",
    "\n",
    "np.mean(rejections) # fraction of tests that correctly reject the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
